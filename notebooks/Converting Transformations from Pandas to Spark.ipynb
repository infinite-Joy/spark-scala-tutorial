{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to setup the spark context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the latest spark https://www.apache.org/dyn/closer.lua/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go Inside and run the spark-shell command. This will download all the relevant jars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SparkContext is a client of Spark’s execution environment and it acts as the master of the Spark application. SparkContext sets up internal services and establishes a connection to a Spark execution environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to avoid hard-coding certain configurations in a SparkConf. For instance, if you’d like to run the same application with different masters or different amounts of memory. Spark allows you to simply create an empty conf:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    val sc = new SparkContext(new SparkConf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can supply configuration values at runtime:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ./bin/spark-submit --name \"My app\" --master local[4] --conf spark.eventLog.enabled=false --conf \"spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\" myApp.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick word on spark tools: sbt, and spark-submit, spark-shell, pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sbt is the built tool for building scala applications.\n",
    "\n",
    "You will need to submit applications to your spark cluster using the spark-submit.\n",
    "\n",
    "Spark-shell will help you in understanding the code execution flow. It is similar to ipython for spark.\n",
    "\n",
    "FInally the bigger question of whether to use scala. According to me the question is do you already have a lot of legacy code in python, and how comfortable is your team to go into the typed environment of scala. Do you believe that strict typing is your friend, because that will be an extra cognitive load. If you ask me, making the upfront investment in typing will help you in your data debugging process. I highly recommend this is the data that you are getting is ambiguous and is likely to change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between a transformation and action\n",
    "\n",
    "In pandas everything is a transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations are executed on demand.(Lazy computation)\n",
    "Ex: filter(), union()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Action will return a non-RDD type (your stored value types usually)\n",
    "Actions triggers execution using lineage graph to load the data into original RDD\n",
    "Ex: count(), first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference: creating a pandas DF and a Spark DF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames generally refer to a data structure, which is tabular in nature. It represents rows, each of which consists of a number of observations. Rows can have a variety of data formats (heterogeneous), whereas a column can have data of the same data type (homogeneous). DataFrames usually contain some metadata in addition to data; for example, column and row names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas:\n",
    "\n",
    "![title](img/pandas_read_csv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "house_prices_df = [Id: int, MSSubClass: int ... 79 more fields]\n",
       "melb_data = [Suburb: string, Address: string ... 19 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Suburb: string, Address: string ... 19 more fields]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val house_prices_df = spark.read\n",
    "    .format(\"csv\")                                    // this is a csv file.\n",
    "    .option(\"header\", \"true\")                         // the file contains headers\n",
    "    .option(\"inferSchema\", true)                      // read the schema\n",
    "    .load(\"/home/jovyan/data/house-prices/train.csv\") // now load the file.\n",
    "\n",
    "val melb_data = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", true)\n",
    "    .load(\"/home/jovyan/data/melbourne_housing_snapshot/melb_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+-----------+-------+------+-----+--------+-----------+---------+---------+---------+------------+----------+----------+--------+----------+-----------+-----------+---------+------------+---------+--------+-----------+-----------+----------+----------+---------+---------+----------+--------+--------+------------+------------+----------+------------+----------+---------+-----------+-------+---------+----------+----------+--------+--------+------------+---------+------------+------------+--------+--------+------------+------------+-----------+------------+----------+----------+-----------+----------+-----------+------------+----------+----------+----------+----------+----------+----------+-----------+-------------+---------+-----------+--------+------+-----+-----------+-------+------+------+--------+-------------+---------+\n",
      "| Id|MSSubClass|MSZoning|LotFrontage|LotArea|Street|Alley|LotShape|LandContour|Utilities|LotConfig|LandSlope|Neighborhood|Condition1|Condition2|BldgType|HouseStyle|OverallQual|OverallCond|YearBuilt|YearRemodAdd|RoofStyle|RoofMatl|Exterior1st|Exterior2nd|MasVnrType|MasVnrArea|ExterQual|ExterCond|Foundation|BsmtQual|BsmtCond|BsmtExposure|BsmtFinType1|BsmtFinSF1|BsmtFinType2|BsmtFinSF2|BsmtUnfSF|TotalBsmtSF|Heating|HeatingQC|CentralAir|Electrical|1stFlrSF|2ndFlrSF|LowQualFinSF|GrLivArea|BsmtFullBath|BsmtHalfBath|FullBath|HalfBath|BedroomAbvGr|KitchenAbvGr|KitchenQual|TotRmsAbvGrd|Functional|Fireplaces|FireplaceQu|GarageType|GarageYrBlt|GarageFinish|GarageCars|GarageArea|GarageQual|GarageCond|PavedDrive|WoodDeckSF|OpenPorchSF|EnclosedPorch|3SsnPorch|ScreenPorch|PoolArea|PoolQC|Fence|MiscFeature|MiscVal|MoSold|YrSold|SaleType|SaleCondition|SalePrice|\n",
      "+---+----------+--------+-----------+-------+------+-----+--------+-----------+---------+---------+---------+------------+----------+----------+--------+----------+-----------+-----------+---------+------------+---------+--------+-----------+-----------+----------+----------+---------+---------+----------+--------+--------+------------+------------+----------+------------+----------+---------+-----------+-------+---------+----------+----------+--------+--------+------------+---------+------------+------------+--------+--------+------------+------------+-----------+------------+----------+----------+-----------+----------+-----------+------------+----------+----------+----------+----------+----------+----------+-----------+-------------+---------+-----------+--------+------+-----+-----------+-------+------+------+--------+-------------+---------+\n",
      "|  1|        60|      RL|         65|   8450|  Pave|   NA|     Reg|        Lvl|   AllPub|   Inside|      Gtl|     CollgCr|      Norm|      Norm|    1Fam|    2Story|          7|          5|     2003|        2003|    Gable| CompShg|    VinylSd|    VinylSd|   BrkFace|       196|       Gd|       TA|     PConc|      Gd|      TA|          No|         GLQ|       706|         Unf|         0|      150|        856|   GasA|       Ex|         Y|     SBrkr|     856|     854|           0|     1710|           1|           0|       2|       1|           3|           1|         Gd|           8|       Typ|         0|         NA|    Attchd|       2003|         RFn|         2|       548|        TA|        TA|         Y|         0|         61|            0|        0|          0|       0|    NA|   NA|         NA|      0|     2|  2008|      WD|       Normal|   208500|\n",
      "|  2|        20|      RL|         80|   9600|  Pave|   NA|     Reg|        Lvl|   AllPub|      FR2|      Gtl|     Veenker|     Feedr|      Norm|    1Fam|    1Story|          6|          8|     1976|        1976|    Gable| CompShg|    MetalSd|    MetalSd|      None|         0|       TA|       TA|    CBlock|      Gd|      TA|          Gd|         ALQ|       978|         Unf|         0|      284|       1262|   GasA|       Ex|         Y|     SBrkr|    1262|       0|           0|     1262|           0|           1|       2|       0|           3|           1|         TA|           6|       Typ|         1|         TA|    Attchd|       1976|         RFn|         2|       460|        TA|        TA|         Y|       298|          0|            0|        0|          0|       0|    NA|   NA|         NA|      0|     5|  2007|      WD|       Normal|   181500|\n",
      "|  3|        60|      RL|         68|  11250|  Pave|   NA|     IR1|        Lvl|   AllPub|   Inside|      Gtl|     CollgCr|      Norm|      Norm|    1Fam|    2Story|          7|          5|     2001|        2002|    Gable| CompShg|    VinylSd|    VinylSd|   BrkFace|       162|       Gd|       TA|     PConc|      Gd|      TA|          Mn|         GLQ|       486|         Unf|         0|      434|        920|   GasA|       Ex|         Y|     SBrkr|     920|     866|           0|     1786|           1|           0|       2|       1|           3|           1|         Gd|           6|       Typ|         1|         TA|    Attchd|       2001|         RFn|         2|       608|        TA|        TA|         Y|         0|         42|            0|        0|          0|       0|    NA|   NA|         NA|      0|     9|  2008|      WD|       Normal|   223500|\n",
      "+---+----------+--------+-----------+-------+------+-----+--------+-----------+---------+---------+---------+------------+----------+----------+--------+----------+-----------+-----------+---------+------------+---------+--------+-----------+-----------+----------+----------+---------+---------+----------+--------+--------+------------+------------+----------+------------+----------+---------+-----------+-------+---------+----------+----------+--------+--------+------------+---------+------------+------------+--------+--------+------------+------------+-----------+------------+----------+----------+-----------+----------+-----------+------------+----------+----------+----------+----------+----------+----------+-----------+-------------+---------+-----------+--------+------+-----+-----------+-------+------+------+--------+-------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_prices_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- MSSubClass: integer (nullable = true)\n",
      " |-- MSZoning: string (nullable = true)\n",
      " |-- LotFrontage: string (nullable = true)\n",
      " |-- LotArea: integer (nullable = true)\n",
      " |-- Street: string (nullable = true)\n",
      " |-- Alley: string (nullable = true)\n",
      " |-- LotShape: string (nullable = true)\n",
      " |-- LandContour: string (nullable = true)\n",
      " |-- Utilities: string (nullable = true)\n",
      " |-- LotConfig: string (nullable = true)\n",
      " |-- LandSlope: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Condition1: string (nullable = true)\n",
      " |-- Condition2: string (nullable = true)\n",
      " |-- BldgType: string (nullable = true)\n",
      " |-- HouseStyle: string (nullable = true)\n",
      " |-- OverallQual: integer (nullable = true)\n",
      " |-- OverallCond: integer (nullable = true)\n",
      " |-- YearBuilt: integer (nullable = true)\n",
      " |-- YearRemodAdd: integer (nullable = true)\n",
      " |-- RoofStyle: string (nullable = true)\n",
      " |-- RoofMatl: string (nullable = true)\n",
      " |-- Exterior1st: string (nullable = true)\n",
      " |-- Exterior2nd: string (nullable = true)\n",
      " |-- MasVnrType: string (nullable = true)\n",
      " |-- MasVnrArea: string (nullable = true)\n",
      " |-- ExterQual: string (nullable = true)\n",
      " |-- ExterCond: string (nullable = true)\n",
      " |-- Foundation: string (nullable = true)\n",
      " |-- BsmtQual: string (nullable = true)\n",
      " |-- BsmtCond: string (nullable = true)\n",
      " |-- BsmtExposure: string (nullable = true)\n",
      " |-- BsmtFinType1: string (nullable = true)\n",
      " |-- BsmtFinSF1: integer (nullable = true)\n",
      " |-- BsmtFinType2: string (nullable = true)\n",
      " |-- BsmtFinSF2: integer (nullable = true)\n",
      " |-- BsmtUnfSF: integer (nullable = true)\n",
      " |-- TotalBsmtSF: integer (nullable = true)\n",
      " |-- Heating: string (nullable = true)\n",
      " |-- HeatingQC: string (nullable = true)\n",
      " |-- CentralAir: string (nullable = true)\n",
      " |-- Electrical: string (nullable = true)\n",
      " |-- 1stFlrSF: integer (nullable = true)\n",
      " |-- 2ndFlrSF: integer (nullable = true)\n",
      " |-- LowQualFinSF: integer (nullable = true)\n",
      " |-- GrLivArea: integer (nullable = true)\n",
      " |-- BsmtFullBath: integer (nullable = true)\n",
      " |-- BsmtHalfBath: integer (nullable = true)\n",
      " |-- FullBath: integer (nullable = true)\n",
      " |-- HalfBath: integer (nullable = true)\n",
      " |-- BedroomAbvGr: integer (nullable = true)\n",
      " |-- KitchenAbvGr: integer (nullable = true)\n",
      " |-- KitchenQual: string (nullable = true)\n",
      " |-- TotRmsAbvGrd: integer (nullable = true)\n",
      " |-- Functional: string (nullable = true)\n",
      " |-- Fireplaces: integer (nullable = true)\n",
      " |-- FireplaceQu: string (nullable = true)\n",
      " |-- GarageType: string (nullable = true)\n",
      " |-- GarageYrBlt: string (nullable = true)\n",
      " |-- GarageFinish: string (nullable = true)\n",
      " |-- GarageCars: integer (nullable = true)\n",
      " |-- GarageArea: integer (nullable = true)\n",
      " |-- GarageQual: string (nullable = true)\n",
      " |-- GarageCond: string (nullable = true)\n",
      " |-- PavedDrive: string (nullable = true)\n",
      " |-- WoodDeckSF: integer (nullable = true)\n",
      " |-- OpenPorchSF: integer (nullable = true)\n",
      " |-- EnclosedPorch: integer (nullable = true)\n",
      " |-- 3SsnPorch: integer (nullable = true)\n",
      " |-- ScreenPorch: integer (nullable = true)\n",
      " |-- PoolArea: integer (nullable = true)\n",
      " |-- PoolQC: string (nullable = true)\n",
      " |-- Fence: string (nullable = true)\n",
      " |-- MiscFeature: string (nullable = true)\n",
      " |-- MiscVal: integer (nullable = true)\n",
      " |-- MoSold: integer (nullable = true)\n",
      " |-- YrSold: integer (nullable = true)\n",
      " |-- SaleType: string (nullable = true)\n",
      " |-- SaleCondition: string (nullable = true)\n",
      " |-- SalePrice: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_prices_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of Spark Dataframes\n",
    "\n",
    "    * DataFrames are distributed in nature, which makes it a fault tolerant and highly available data structure.\n",
    "    * Lazy evaluation is an evaluation strategy which holds the evaluation of an expression until its value is needed. It avoids repeated evaluation. Lazy evaluation in Spark means that the execution will not start until an action is triggered. In Spark, the picture of lazy evaluation comes when Spark transformations occur.\n",
    "    * DataFrames are immutable in nature. By immutable, I mean that it is an object whose state cannot be modified after it is created. But we can transform its values by applying a certain transformation, like in RDDs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing a particular column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![Describing in pandas](./img/pandas_describe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        MSSubClass|\n",
      "+-------+------------------+\n",
      "|  count|              1460|\n",
      "|   mean|56.897260273972606|\n",
      "| stddev| 42.30057099381045|\n",
      "|    min|                20|\n",
      "|    max|               190|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_prices_df.describe(\"MSSubClass\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataframe shape and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/pandas_shape_columns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the number of samples is an action and hence be mindful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1460"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_prices_df.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the number of features is a no big deal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_prices_df.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Id, MSSubClass, MSZoning, LotFrontage, LotArea, Street, Alley, LotShape, LandContour, Utilities, LotConfig, LandSlope, Neighborhood, Condition1, Condition2, BldgType, HouseStyle, OverallQual, OverallCond, YearBuilt, YearRemodAdd, RoofStyle, RoofMatl, Exterior1st, Exterior2nd, MasVnrType, MasVnrArea, ExterQual, ExterCond, Foundation, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinSF1, BsmtFinType2, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, Heating, HeatingQC, CentralAir, Electrical, 1stFlrSF, 2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, KitchenQual, TotRmsAbvGrd, Functional, Fireplaces, FireplaceQu, GarageType, GarageYrBlt, GarageFinish, GarageCars, GarageArea, GarageQual, GarageCond, PavedDr..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_prices_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the column names of the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/change_column_names.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = false)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [_1: bigint, _2: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_1: bigint, _2: string ... 2 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq((1L, \"a\", \"foo\", 3.0)).toDF\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id| x1| x2| x3|\n",
      "+---+---+---+---+\n",
      "|  1|  a|foo|3.0|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newNames = List(id, x1, x2, x3)\n",
       "dfRenamed = [id: bigint, x1: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: bigint, x1: string ... 2 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newNames = Seq(\"id\", \"x1\", \"x2\", \"x3\")\n",
    "val dfRenamed = df.toDF(newNames: _*)\n",
    "\n",
    "dfRenamed.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lookup = Map(Id -> id, SalePrice -> SalePriceDollars)\n",
       "changed_cols_df = [id: int, MSSubClass: int ... 79 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: int, MSSubClass: int ... 79 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val lookup = Map(\"Id\" -> \"id\", \"SalePrice\" -> \"SalePriceDollars\")\n",
    "\n",
    "val changed_cols_df = house_prices_df.select(\n",
    "    house_prices_df.columns\n",
    "    .map(\n",
    "        c => col(c).as(lookup.getOrElse(c, c))\n",
    "    ): _*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- MSSubClass: integer (nullable = true)\n",
      " |-- MSZoning: string (nullable = true)\n",
      " |-- LotFrontage: string (nullable = true)\n",
      " |-- LotArea: integer (nullable = true)\n",
      " |-- Street: string (nullable = true)\n",
      " |-- Alley: string (nullable = true)\n",
      " |-- LotShape: string (nullable = true)\n",
      " |-- LandContour: string (nullable = true)\n",
      " |-- Utilities: string (nullable = true)\n",
      " |-- LotConfig: string (nullable = true)\n",
      " |-- LandSlope: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Condition1: string (nullable = true)\n",
      " |-- Condition2: string (nullable = true)\n",
      " |-- BldgType: string (nullable = true)\n",
      " |-- HouseStyle: string (nullable = true)\n",
      " |-- OverallQual: integer (nullable = true)\n",
      " |-- OverallCond: integer (nullable = true)\n",
      " |-- YearBuilt: integer (nullable = true)\n",
      " |-- YearRemodAdd: integer (nullable = true)\n",
      " |-- RoofStyle: string (nullable = true)\n",
      " |-- RoofMatl: string (nullable = true)\n",
      " |-- Exterior1st: string (nullable = true)\n",
      " |-- Exterior2nd: string (nullable = true)\n",
      " |-- MasVnrType: string (nullable = true)\n",
      " |-- MasVnrArea: string (nullable = true)\n",
      " |-- ExterQual: string (nullable = true)\n",
      " |-- ExterCond: string (nullable = true)\n",
      " |-- Foundation: string (nullable = true)\n",
      " |-- BsmtQual: string (nullable = true)\n",
      " |-- BsmtCond: string (nullable = true)\n",
      " |-- BsmtExposure: string (nullable = true)\n",
      " |-- BsmtFinType1: string (nullable = true)\n",
      " |-- BsmtFinSF1: integer (nullable = true)\n",
      " |-- BsmtFinType2: string (nullable = true)\n",
      " |-- BsmtFinSF2: integer (nullable = true)\n",
      " |-- BsmtUnfSF: integer (nullable = true)\n",
      " |-- TotalBsmtSF: integer (nullable = true)\n",
      " |-- Heating: string (nullable = true)\n",
      " |-- HeatingQC: string (nullable = true)\n",
      " |-- CentralAir: string (nullable = true)\n",
      " |-- Electrical: string (nullable = true)\n",
      " |-- 1stFlrSF: integer (nullable = true)\n",
      " |-- 2ndFlrSF: integer (nullable = true)\n",
      " |-- LowQualFinSF: integer (nullable = true)\n",
      " |-- GrLivArea: integer (nullable = true)\n",
      " |-- BsmtFullBath: integer (nullable = true)\n",
      " |-- BsmtHalfBath: integer (nullable = true)\n",
      " |-- FullBath: integer (nullable = true)\n",
      " |-- HalfBath: integer (nullable = true)\n",
      " |-- BedroomAbvGr: integer (nullable = true)\n",
      " |-- KitchenAbvGr: integer (nullable = true)\n",
      " |-- KitchenQual: string (nullable = true)\n",
      " |-- TotRmsAbvGrd: integer (nullable = true)\n",
      " |-- Functional: string (nullable = true)\n",
      " |-- Fireplaces: integer (nullable = true)\n",
      " |-- FireplaceQu: string (nullable = true)\n",
      " |-- GarageType: string (nullable = true)\n",
      " |-- GarageYrBlt: string (nullable = true)\n",
      " |-- GarageFinish: string (nullable = true)\n",
      " |-- GarageCars: integer (nullable = true)\n",
      " |-- GarageArea: integer (nullable = true)\n",
      " |-- GarageQual: string (nullable = true)\n",
      " |-- GarageCond: string (nullable = true)\n",
      " |-- PavedDrive: string (nullable = true)\n",
      " |-- WoodDeckSF: integer (nullable = true)\n",
      " |-- OpenPorchSF: integer (nullable = true)\n",
      " |-- EnclosedPorch: integer (nullable = true)\n",
      " |-- 3SsnPorch: integer (nullable = true)\n",
      " |-- ScreenPorch: integer (nullable = true)\n",
      " |-- PoolArea: integer (nullable = true)\n",
      " |-- PoolQC: string (nullable = true)\n",
      " |-- Fence: string (nullable = true)\n",
      " |-- MiscFeature: string (nullable = true)\n",
      " |-- MiscVal: integer (nullable = true)\n",
      " |-- MoSold: integer (nullable = true)\n",
      " |-- YrSold: integer (nullable = true)\n",
      " |-- SaleType: string (nullable = true)\n",
      " |-- SaleCondition: string (nullable = true)\n",
      " |-- SalePriceDollars: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "changed_cols_df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/unique_values.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|MSSubClass|\n",
      "+----------+\n",
      "|        85|\n",
      "|       190|\n",
      "|        20|\n",
      "|        40|\n",
      "|       120|\n",
      "|        50|\n",
      "|        45|\n",
      "|        80|\n",
      "|       160|\n",
      "|        70|\n",
      "|        60|\n",
      "|        90|\n",
      "|        75|\n",
      "|        30|\n",
      "|       180|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_prices_df.select(\"MSSubClass\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large data when you are only interested in the count of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|count(DISTINCT MSSubClass)|\n",
      "+--------------------------+\n",
      "|                        15|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.countDistinct\n",
    "\n",
    "house_prices_df.select(countDistinct('MSSubClass)).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do count approximate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Counts\n",
    "\n",
    "ref: https://stackoverflow.com/a/37949565/5417164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/value_counts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|MSSubClass|count|\n",
      "+----------+-----+\n",
      "|        20|  536|\n",
      "|        60|  299|\n",
      "|        50|  144|\n",
      "|       120|   87|\n",
      "|        30|   69|\n",
      "|       160|   63|\n",
      "|        70|   60|\n",
      "|        80|   58|\n",
      "|        90|   52|\n",
      "|       190|   30|\n",
      "|        85|   20|\n",
      "|        75|   16|\n",
      "|        45|   12|\n",
      "|       180|   10|\n",
      "|        40|    4|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "house_prices_df\n",
    "    .groupBy(\"MSSubClass\")  // groupby your class\n",
    "    .count()                // count the values, this should create a dedicated count column\n",
    "    .orderBy($\"count\" desc)       // orderby the count column\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by and group by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/orderbuy_grpby.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|MSSubClass|numOccurances|\n",
      "+----------+-------------+\n",
      "|        20|          536|\n",
      "|        60|          299|\n",
      "|        50|          144|\n",
      "|       120|           87|\n",
      "|        30|           69|\n",
      "|       160|           63|\n",
      "|        70|           60|\n",
      "|        80|           58|\n",
      "|        90|           52|\n",
      "|       190|           30|\n",
      "|        85|           20|\n",
      "|        75|           16|\n",
      "|        45|           12|\n",
      "|       180|           10|\n",
      "|        40|            4|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "house_prices_df\n",
    "    .groupBy($\"MSSubClass\")                   // Count number of occurrences of each word\n",
    "    .agg(count(\"*\") as \"numOccurances\")       // SQL: SELECT COUNT(DISTINCT MSSubClass) AS numOccurances FROM house_prices_df\n",
    "    .orderBy($\"numOccurances\" desc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/filtering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "highSubClass = [Id: int, MSSubClass: int ... 79 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Id: int, MSSubClass: int ... 79 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val highSubClass = house_prices_df.filter($\"MSSubClass\" > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+-----------+-------+------+-----+--------+-----------+---------+---------+---------+------------+----------+----------+--------+----------+-----------+-----------+---------+------------+---------+--------+-----------+-----------+----------+----------+---------+---------+----------+--------+--------+------------+------------+----------+------------+----------+---------+-----------+-------+---------+----------+----------+--------+--------+------------+---------+------------+------------+--------+--------+------------+------------+-----------+------------+----------+----------+-----------+----------+-----------+------------+----------+----------+----------+----------+----------+----------+-----------+-------------+---------+-----------+--------+------+-----+-----------+-------+------+------+--------+-------------+---------+\n",
      "| Id|MSSubClass|MSZoning|LotFrontage|LotArea|Street|Alley|LotShape|LandContour|Utilities|LotConfig|LandSlope|Neighborhood|Condition1|Condition2|BldgType|HouseStyle|OverallQual|OverallCond|YearBuilt|YearRemodAdd|RoofStyle|RoofMatl|Exterior1st|Exterior2nd|MasVnrType|MasVnrArea|ExterQual|ExterCond|Foundation|BsmtQual|BsmtCond|BsmtExposure|BsmtFinType1|BsmtFinSF1|BsmtFinType2|BsmtFinSF2|BsmtUnfSF|TotalBsmtSF|Heating|HeatingQC|CentralAir|Electrical|1stFlrSF|2ndFlrSF|LowQualFinSF|GrLivArea|BsmtFullBath|BsmtHalfBath|FullBath|HalfBath|BedroomAbvGr|KitchenAbvGr|KitchenQual|TotRmsAbvGrd|Functional|Fireplaces|FireplaceQu|GarageType|GarageYrBlt|GarageFinish|GarageCars|GarageArea|GarageQual|GarageCond|PavedDrive|WoodDeckSF|OpenPorchSF|EnclosedPorch|3SsnPorch|ScreenPorch|PoolArea|PoolQC|Fence|MiscFeature|MiscVal|MoSold|YrSold|SaleType|SaleCondition|SalePrice|\n",
      "+---+----------+--------+-----------+-------+------+-----+--------+-----------+---------+---------+---------+------------+----------+----------+--------+----------+-----------+-----------+---------+------------+---------+--------+-----------+-----------+----------+----------+---------+---------+----------+--------+--------+------------+------------+----------+------------+----------+---------+-----------+-------+---------+----------+----------+--------+--------+------------+---------+------------+------------+--------+--------+------------+------------+-----------+------------+----------+----------+-----------+----------+-----------+------------+----------+----------+----------+----------+----------+----------+-----------+-------------+---------+-----------+--------+------+-----+-----------+-------+------+------+--------+-------------+---------+\n",
      "| 10|       190|      RL|         50|   7420|  Pave|   NA|     Reg|        Lvl|   AllPub|   Corner|      Gtl|     BrkSide|    Artery|    Artery|  2fmCon|    1.5Unf|          5|          6|     1939|        1950|    Gable| CompShg|    MetalSd|    MetalSd|      None|         0|       TA|       TA|    BrkTil|      TA|      TA|          No|         GLQ|       851|         Unf|         0|      140|        991|   GasA|       Ex|         Y|     SBrkr|    1077|       0|           0|     1077|           1|           0|       1|       0|           2|           2|         TA|           5|       Typ|         2|         TA|    Attchd|       1939|         RFn|         1|       205|        Gd|        TA|         Y|         0|          4|            0|        0|          0|       0|    NA|   NA|         NA|      0|     1|  2008|      WD|       Normal|   118000|\n",
      "| 24|       120|      RM|         44|   4224|  Pave|   NA|     Reg|        Lvl|   AllPub|   Inside|      Gtl|     MeadowV|      Norm|      Norm|  TwnhsE|    1Story|          5|          7|     1976|        1976|    Gable| CompShg|    CemntBd|    CmentBd|      None|         0|       TA|       TA|     PConc|      Gd|      TA|          No|         GLQ|       840|         Unf|         0|      200|       1040|   GasA|       TA|         Y|     SBrkr|    1060|       0|           0|     1060|           1|           0|       1|       0|           3|           1|         TA|           6|       Typ|         1|         TA|    Attchd|       1976|         Unf|         2|       572|        TA|        TA|         Y|       100|        110|            0|        0|          0|       0|    NA|   NA|         NA|      0|     6|  2007|      WD|       Normal|   129900|\n",
      "| 35|       120|      RL|         60|   7313|  Pave|   NA|     Reg|        Lvl|   AllPub|   Inside|      Gtl|     NridgHt|      Norm|      Norm|  TwnhsE|    1Story|          9|          5|     2005|        2005|      Hip| CompShg|    MetalSd|    MetalSd|   BrkFace|       246|       Ex|       TA|     PConc|      Ex|      TA|          No|         GLQ|      1153|         Unf|         0|      408|       1561|   GasA|       Ex|         Y|     SBrkr|    1561|       0|           0|     1561|           1|           0|       2|       0|           2|           1|         Ex|           6|       Typ|         1|         Gd|    Attchd|       2005|         Fin|         2|       556|        TA|        TA|         Y|       203|         47|            0|        0|          0|       0|    NA|   NA|         NA|      0|     8|  2007|      WD|       Normal|   277500|\n",
      "+---+----------+--------+-----------+-------+------+-----+--------+-----------+---------+---------+---------+------------+----------+----------+--------+----------+-----------+-----------+---------+------------+---------+--------+-----------+-----------+----------+----------+---------+---------+----------+--------+--------+------------+------------+----------+------------+----------+---------+-----------+-------+---------+----------+----------+--------+--------+------------+---------+------------+------------+--------+--------+------------+------------+-----------+------------+----------+----------+-----------+----------+-----------+------------+----------+----------+----------+----------+----------+----------+-----------+-------------+---------+-----------+--------+------+-----+-----------+-------+------+------+--------+-------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "highSubClass.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membership in dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/membership.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "presentList = List(20, 60)\n",
       "nopresentList = List(20000)\n",
       "validMembership = [Id: int, MSSubClass: int ... 79 more fields]\n",
       "invalidMembership = [Id: int, MSSubClass: int ... 79 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Id: int, MSSubClass: int ... 79 more fields]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val presentList = List(\"20\",\"60\") \n",
    "val nopresentList = List(\"20000\") \n",
    "val validMembership = house_prices_df.filter($\"MSSubClass\".isin(presentList:_*))\n",
    "val invalidMembership = house_prices_df.filter($\"MSSubClass\".isin(nopresentList:_*))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the count below is the sum of 536 + 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validMembership.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalidMembership.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value imputation\n",
    "\n",
    "refs: https://stackoverflow.com/a/40059453/5417164 \n",
    "\n",
    "https://medium.com/@mrpowers/dealing-with-null-in-spark-cfdbb12f231e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Rooms: integer (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Method: string (nullable = true)\n",
      " |-- SellerG: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- Postcode: double (nullable = true)\n",
      " |-- Bedroom2: double (nullable = true)\n",
      " |-- Bathroom: double (nullable = true)\n",
      " |-- Car: double (nullable = true)\n",
      " |-- Landsize: double (nullable = true)\n",
      " |-- BuildingArea: double (nullable = true)\n",
      " |-- YearBuilt: double (nullable = true)\n",
      " |-- CouncilArea: string (nullable = true)\n",
      " |-- Lattitude: double (nullable = true)\n",
      " |-- Longtitude: double (nullable = true)\n",
      " |-- Regionname: string (nullable = true)\n",
      " |-- Propertycount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "melb_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----+----+---------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+-----------+---------+----------+--------------------+-------------+\n",
      "|    Suburb|        Address|Rooms|Type|    Price|Method|SellerG|     Date|Distance|Postcode|Bedroom2|Bathroom|Car|Landsize|BuildingArea|YearBuilt|CouncilArea|Lattitude|Longtitude|          Regionname|Propertycount|\n",
      "+----------+---------------+-----+----+---------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+-----------+---------+----------+--------------------+-------------+\n",
      "|Abbotsford|   85 Turner St|    2|   h|1480000.0|     S| Biggin|3/12/2016|     2.5|  3067.0|     2.0|     1.0|1.0|   202.0|        null|     null|      Yarra| -37.7996|  144.9984|Northern Metropol...|       4019.0|\n",
      "|Abbotsford|25 Bloomburg St|    2|   h|1035000.0|     S| Biggin|4/02/2016|     2.5|  3067.0|     2.0|     1.0|0.0|   156.0|        79.0|   1900.0|      Yarra| -37.8079|  144.9934|Northern Metropol...|       4019.0|\n",
      "|Abbotsford|   5 Charles St|    3|   h|1465000.0|    SP| Biggin|4/03/2017|     2.5|  3067.0|     3.0|     2.0|0.0|   134.0|       150.0|   1900.0|      Yarra| -37.8093|  144.9944|Northern Metropol...|       4019.0|\n",
      "+----------+---------------+-----+----+---------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+-----------+---------+----------+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "melb_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/fillna.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to pandas you can replace the na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "imputed_melb_data = [Suburb: string, Address: string ... 19 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Suburb: string, Address: string ... 19 more fields]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val imputed_melb_data = melb_data.na.fill(1964.0, Seq(\"YearBuilt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-----+----+---------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+-----------+---------+----------+--------------------+-------------+\n",
      "|    Suburb|         Address|Rooms|Type|    Price|Method|SellerG|     Date|Distance|Postcode|Bedroom2|Bathroom|Car|Landsize|BuildingArea|YearBuilt|CouncilArea|Lattitude|Longtitude|          Regionname|Propertycount|\n",
      "+----------+----------------+-----+----+---------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+-----------+---------+----------+--------------------+-------------+\n",
      "|Abbotsford|    85 Turner St|    2|   h|1480000.0|     S| Biggin|3/12/2016|     2.5|  3067.0|     2.0|     1.0|1.0|   202.0|        null|   1964.0|      Yarra| -37.7996|  144.9984|Northern Metropol...|       4019.0|\n",
      "|Abbotsford| 25 Bloomburg St|    2|   h|1035000.0|     S| Biggin|4/02/2016|     2.5|  3067.0|     2.0|     1.0|0.0|   156.0|        79.0|   1900.0|      Yarra| -37.8079|  144.9934|Northern Metropol...|       4019.0|\n",
      "|Abbotsford|    5 Charles St|    3|   h|1465000.0|    SP| Biggin|4/03/2017|     2.5|  3067.0|     3.0|     2.0|0.0|   134.0|       150.0|   1900.0|      Yarra| -37.8093|  144.9944|Northern Metropol...|       4019.0|\n",
      "|Abbotsford|40 Federation La|    3|   h| 850000.0|    PI| Biggin|4/03/2017|     2.5|  3067.0|     3.0|     2.0|1.0|    94.0|        null|   1964.0|      Yarra| -37.7969|  144.9969|Northern Metropol...|       4019.0|\n",
      "|Abbotsford|     55a Park St|    4|   h|1600000.0|    VB| Nelson|4/06/2016|     2.5|  3067.0|     3.0|     1.0|2.0|   120.0|       142.0|   2014.0|      Yarra| -37.8072|  144.9941|Northern Metropol...|       4019.0|\n",
      "+----------+----------------+-----+----+---------+------+-------+---------+--------+--------+--------+--------+---+--------+------------+---------+-----------+---------+----------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputed_melb_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/imputer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.IllegalArgumentException\n",
       "Message: requirement failed: Column Rooms must be of type equal to one of the following types: [DoubleType, FloatType] but was actually of type IntegerType.\n",
       "StackTrace:   at scala.Predef$.require(Predef.scala:224)\n",
       "  at org.apache.spark.ml.util.SchemaUtils$.checkColumnTypes(SchemaUtils.scala:58)\n",
       "  at org.apache.spark.ml.feature.ImputerParams$$anonfun$2.apply(Imputer.scala:76)\n",
       "  at org.apache.spark.ml.feature.ImputerParams$$anonfun$2.apply(Imputer.scala:74)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
       "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n",
       "  at org.apache.spark.ml.feature.ImputerParams$class.validateAndTransformSchema(Imputer.scala:74)\n",
       "  at org.apache.spark.ml.feature.Imputer.validateAndTransformSchema(Imputer.scala:96)\n",
       "  at org.apache.spark.ml.feature.Imputer.transformSchema(Imputer.scala:175)\n",
       "  at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n",
       "  at org.apache.spark.ml.feature.Imputer.fit(Imputer.scala:124)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Imputer\n",
    "\n",
    "val features_in_focus = Array(\"Rooms\", \"Bathroom\", \"Landsize\", \"BuildingArea\",\n",
    "                              \"YearBuilt\", \"Lattitude\", \"Longtitude\")\n",
    "\n",
    "val imputer = new Imputer()\n",
    "  .setInputCols(features_in_focus)\n",
    "  .setOutputCols(features_in_focus.map(c => s\"${c}_imputed\"))\n",
    "  .setStrategy(\"mean\")\n",
    "\n",
    "imputer.fit(melb_data).transform(melb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above the features must be for double type or floattype. But the Rooms feature is of type Integer and hence we will need to convert that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "melb_data2 = [Suburb: string, Address: string ... 19 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Suburb: string, Address: string ... 19 more fields]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.DoubleType\n",
    "\n",
    "val melb_data2 = melb_data\n",
    "    .withColumn(\"_Rooms\", melb_data(\"Rooms\").cast(DoubleType))\n",
    "    .drop(\"Rooms\")\n",
    "    .withColumnRenamed(\"_Rooms\", \"Rooms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+----------------+--------------------+------------------+-----------------+------------------+\n",
      "|Rooms_imputed|Bathroom_imputed|Landsize_imputed|BuildingArea_imputed| YearBuilt_imputed|Lattitude_imputed|Longtitude_imputed|\n",
      "+-------------+----------------+----------------+--------------------+------------------+-----------------+------------------+\n",
      "|          2.0|             1.0|           202.0|  151.96764988779805|1964.6842169408897|         -37.7996|          144.9984|\n",
      "|          2.0|             1.0|           156.0|                79.0|            1900.0|         -37.8079|          144.9934|\n",
      "|          3.0|             2.0|           134.0|               150.0|            1900.0|         -37.8093|          144.9944|\n",
      "|          3.0|             2.0|            94.0|  151.96764988779805|1964.6842169408897|         -37.7969|          144.9969|\n",
      "|          4.0|             1.0|           120.0|               142.0|            2014.0|         -37.8072|          144.9941|\n",
      "+-------------+----------------+----------------+--------------------+------------------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "features_in_focus = Array(Rooms, Bathroom, Landsize, BuildingArea, YearBuilt, Lattitude, Longtitude)\n",
       "features_in_focus_imputed = Array(Rooms_imputed, Bathroom_imputed, Landsize_imputed, BuildingArea_imputed, YearBuilt_imputed, Lattitude_imputed, Longtitude_imputed)\n",
       "imputer = imputer_fd2f4820db4f\n",
       "imputed_melb_data = [Suburb: string, Address: string ... 26 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Suburb: string, Address: string ... 26 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Imputer\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val features_in_focus = Array(\"Rooms\", \"Bathroom\", \"Landsize\", \"BuildingArea\",\n",
    "                              \"YearBuilt\", \"Lattitude\", \"Longtitude\")\n",
    "val features_in_focus_imputed = features_in_focus.map(c => s\"${c}_imputed\")\n",
    "\n",
    "val imputer = new Imputer()\n",
    "  .setInputCols(features_in_focus)\n",
    "  .setOutputCols(features_in_focus_imputed)\n",
    "  .setStrategy(\"mean\")\n",
    "\n",
    "val imputed_melb_data = imputer.fit(melb_data2).transform(melb_data2)\n",
    "\n",
    "imputed_melb_data.select(features_in_focus_imputed.map(name => col(name)):_*).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization and Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/binning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing the dataframe\n",
      "+--------+\n",
      "|features|\n",
      "+--------+\n",
      "|  -999.9|\n",
      "|    -0.5|\n",
      "|    -0.3|\n",
      "|     0.0|\n",
      "|     0.2|\n",
      "|   999.9|\n",
      "+--------+\n",
      "\n",
      "Bucketizer output with 4 buckets\n",
      "+--------+----------------+\n",
      "|features|bucketedFeatures|\n",
      "+--------+----------------+\n",
      "|  -999.9|             0.0|\n",
      "|    -0.5|             1.0|\n",
      "|    -0.3|             1.0|\n",
      "|     0.0|             2.0|\n",
      "|     0.2|             2.0|\n",
      "|   999.9|             3.0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "splits = Array(-Infinity, -0.5, 0.0, 0.5, Infinity)\n",
       "data = Array(-999.9, -0.5, -0.3, 0.0, 0.2, 999.9)\n",
       "dataFrame = [features: double]\n",
       "bucketizer = bucketizer_afd411a4ab1f\n",
       "bucketedData = [features: double, bucketedFeatures: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: double, bucketedFeatures: double]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Bucketizer\n",
    "\n",
    "val splits = Array(Double.NegativeInfinity, -0.5, 0.0, 0.5, Double.PositiveInfinity)\n",
    "\n",
    "val data = Array(-999.9, -0.5, -0.3, 0.0, 0.2, 999.9)\n",
    "val dataFrame = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "\n",
    "println(\"showing the dataframe\")\n",
    "dataFrame.show()\n",
    "\n",
    "val bucketizer = new Bucketizer()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"bucketedFeatures\")\n",
    "  .setSplits(splits)\n",
    "\n",
    "// Transform original data into its bucket index.\n",
    "val bucketedData = bucketizer.transform(dataFrame)\n",
    "\n",
    "println(s\"Bucketizer output with ${bucketizer.getSplits.length-1} buckets\")\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing the dataframe\n",
      "+--------+\n",
      "|features|\n",
      "+--------+\n",
      "|    20.0|\n",
      "|    22.0|\n",
      "|    25.0|\n",
      "|    27.0|\n",
      "|    21.0|\n",
      "|    23.0|\n",
      "|    37.0|\n",
      "|    31.0|\n",
      "|    61.0|\n",
      "|    45.0|\n",
      "|    41.0|\n",
      "|    32.0|\n",
      "+--------+\n",
      "\n",
      "Bucketizer output with 4 buckets\n",
      "+--------+----------------+\n",
      "|features|bucketedFeatures|\n",
      "+--------+----------------+\n",
      "|    20.0|             0.0|\n",
      "|    22.0|             0.0|\n",
      "|    25.0|             1.0|\n",
      "|    27.0|             1.0|\n",
      "|    21.0|             0.0|\n",
      "|    23.0|             0.0|\n",
      "|    37.0|             2.0|\n",
      "|    31.0|             1.0|\n",
      "|    61.0|             3.0|\n",
      "|    45.0|             2.0|\n",
      "|    41.0|             2.0|\n",
      "|    32.0|             1.0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = Array(20.0, 22.0, 25.0, 27.0, 21.0, 23.0, 37.0, 31.0, 61.0, 45.0, 41.0, 32.0)\n",
       "bins = Array(18.0, 25.0, 35.0, 60.0, 100.0)\n",
       "dataFrame = [features: double]\n",
       "bucketizer = bucketizer_a14e358ca67c\n",
       "bucketedData = [features: double, bucketedFeatures: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[features: double, bucketedFeatures: double]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Array(20.0, 22.0, 25.0, 27.0, 21.0, 23.0, 37.0, 31.0, 61.0, 45.0, 41.0, 32.0)\n",
    "val bins = Array(18.0, 25.0, 35.0, 60.0, 100.0)\n",
    "\n",
    "// val dataFrame = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "val dataFrame = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "\n",
    "println(\"showing the dataframe\")\n",
    "dataFrame.show()\n",
    "\n",
    "val bucketizer = new Bucketizer()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"bucketedFeatures\")\n",
    "  .setSplits(bins)\n",
    "\n",
    "// Transform original data into its bucket index.\n",
    "val bucketedData = bucketizer.transform(dataFrame)\n",
    "\n",
    "println(s\"Bucketizer output with ${bucketizer.getSplits.length-1} buckets\")\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a particular data\n",
    "\n",
    "ref: https://stackoverflow.com/a/35720457/5417164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/iloc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result = Array([60])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array([60])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = house_prices_df.\n",
    "    filter(line => line(0) == 1)\n",
    "    .select(\"MSSubClass\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([60])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting\n",
    "\n",
    "ref: https://stackoverflow.com/a/32052881/5417164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/sorting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sortedbyMsSubclass = [Id: int, MSSubClass: int ... 79 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Id: int, MSSubClass: int ... 79 more fields]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val sortedbyMsSubclass = house_prices_df.sort(desc(\"MSSubClass\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|MSSubClass|SalePrice|\n",
      "+----------+---------+\n",
      "|       190|   133900|\n",
      "|       190|   137000|\n",
      "|       190|    84500|\n",
      "+----------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "columnNames = List(MSSubClass, SalePrice)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(MSSubClass, SalePrice)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columnNames = Seq(\"MSSubClass\", \"SalePrice\")\n",
    "sortedbyMsSubclass.select(columnNames.map(c => col(c)): _*).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping and Pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/pivot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------+------+\n",
      "|             product|category|quarter|profit|\n",
      "+--------------------+--------+-------+------+\n",
      "|            memories|    book|     q1|    10|\n",
      "|              dreams|    book|     q2|    20|\n",
      "|         reflections|    book|     q3|    30|\n",
      "|how to build a house|    book|     q4|    40|\n",
      "|      wonderful life|   music|     q1|    10|\n",
      "|       million miles|   music|     q2|    20|\n",
      "|            run away|   music|     q3|    30|\n",
      "|       mind and body|   music|     q4|    40|\n",
      "+--------------------+--------+-------+------+\n",
      "\n",
      "+--------+---+---+---+---+\n",
      "|category| q1| q2| q3| q4|\n",
      "+--------+---+---+---+---+\n",
      "|   music| 10| 20| 30| 40|\n",
      "|    book| 10| 20| 30| 40|\n",
      "+--------+---+---+---+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = ParallelCollectionRDD[105] at parallelize at <console>:43\n",
       "df_products = [product: string, category: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[product: string, category: string ... 2 more fields]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create RDD for products\n",
    "val data = sc.parallelize(Seq(\n",
    "    (\"memories\",\"book\",\"q1\",10),\n",
    "    (\"dreams\",\"book\",\"q2\",20),\n",
    "    (\"reflections\",\"book\",\"q3\",30),\n",
    "    (\"how to build a house\",\"book\",\"q4\",40),\n",
    "    (\"wonderful life\",\"music\",\"q1\",10),\n",
    "    (\"million miles\",\"music\",\"q2\",20),\n",
    "    (\"run away\",\"music\",\"q3\",30),\n",
    "    (\"mind and body\",\"music\",\"q4\",40)\n",
    "))\n",
    "\n",
    "// convert the RDD to DataFrame\n",
    "val df_products = spark.createDataFrame(data).toDF(\"product\",\"category\",\"quarter\",\"profit\")\n",
    "df_products.show()\n",
    "\n",
    "// index column : category\n",
    "// value column : profit\n",
    "// pivot column : quarter\n",
    "// agg function : sum\n",
    "\n",
    "// apply pivot on DataFrame DataFrame\n",
    "df_products.groupBy(\"category\").pivot(\"quarter\").sum(\"profit\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merges and Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/merges.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+------+\n",
      "| name|      date|duration|upload|\n",
      "+-----+----------+--------+------+\n",
      "|  bob|2015-01-13|       4|    23|\n",
      "|alice|2015-04-23|      10|   100|\n",
      "+-----+----------+--------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "llist = List((bob,2015-01-13,4), (alice,2015-04-23,10))\n",
       "left = [name: string, date: string ... 1 more field]\n",
       "right = [name: string, upload: int]\n",
       "df = [name: string, date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[name: string, date: string ... 2 more fields]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val llist = Seq((\"bob\", \"2015-01-13\", 4), (\"alice\", \"2015-04-23\",10))\n",
    "val left = llist.toDF(\"name\",\"date\",\"duration\")\n",
    "val right = Seq((\"alice\", 100),(\"bob\", 23)).toDF(\"name\",\"upload\")\n",
    "\n",
    "val df = left.join(right, Seq(\"name\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating and appending to the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/append.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+\n",
      "| name|      date|duration|\n",
      "+-----+----------+--------+\n",
      "|  bob|2015-01-13|       4|\n",
      "|alice|2015-04-23|      10|\n",
      "|  bob|2015-01-13|       4|\n",
      "|alice|2015-04-23|      10|\n",
      "+-----+----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "llist = List((bob,2015-01-13,4), (alice,2015-04-23,10))\n",
       "arr = [name: string, date: string ... 1 more field]\n",
       "appended = [name: string, date: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[name: string, date: string ... 1 more field]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val llist = Seq((\"bob\", \"2015-01-13\", 4), (\"alice\", \"2015-04-23\",10))\n",
    "val arr = llist.toDF(\"name\",\"date\",\"duration\")\n",
    "val appended = arr.union(arr.toDF())\n",
    "appended.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function application, transformations and mapping\n",
    "\n",
    "By using user defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "morePrecisionUdf = UserDefinedFunction(<function1>,FloatType,Some(List(IntegerType)))\n",
       "converted_df = [UDF(SalePrice): float]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "morePrecision: (price: Integer)Float\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[UDF(SalePrice): float]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def morePrecision(price: Integer): Float = price.toFloat\n",
    "\n",
    "// we use the method name followed by a \"_\" to indicate we want a reference\n",
    "// to the method, not call it\n",
    "val morePrecisionUdf = udf(morePrecision _)\n",
    "\n",
    "val converted_df = house_prices_df.select(morePrecisionUdf(house_prices_df(\"SalePrice\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|UDF(SalePrice)|\n",
      "+--------------+\n",
      "|      208500.0|\n",
      "|      181500.0|\n",
      "|      223500.0|\n",
      "+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "converted_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the udf on the same df. ie. creating a new feature by transforming another column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+-----------+-------+------+-----+--------+-----------+---------+---------+---------+------------+----------+----------+--------+----------+-----------+-----------+---------+------------+---------+--------+-----------+-----------+----------+----------+---------+---------+----------+--------+--------+------------+------------+----------+------------+----------+---------+-----------+-------+---------+----------+----------+--------+--------+------------+---------+------------+------------+--------+--------+------------+------------+-----------+------------+----------+----------+-----------+----------+-----------+------------+----------+----------+----------+----------+----------+----------+-----------+-------------+---------+-----------+--------+------+-----+-----------+-------+------+------+--------+-------------+---------+----------------------+\n",
      "| Id|MSSubClass|MSZoning|LotFrontage|LotArea|Street|Alley|LotShape|LandContour|Utilities|LotConfig|LandSlope|Neighborhood|Condition1|Condition2|BldgType|HouseStyle|OverallQual|OverallCond|YearBuilt|YearRemodAdd|RoofStyle|RoofMatl|Exterior1st|Exterior2nd|MasVnrType|MasVnrArea|ExterQual|ExterCond|Foundation|BsmtQual|BsmtCond|BsmtExposure|BsmtFinType1|BsmtFinSF1|BsmtFinType2|BsmtFinSF2|BsmtUnfSF|TotalBsmtSF|Heating|HeatingQC|CentralAir|Electrical|1stFlrSF|2ndFlrSF|LowQualFinSF|GrLivArea|BsmtFullBath|BsmtHalfBath|FullBath|HalfBath|BedroomAbvGr|KitchenAbvGr|KitchenQual|TotRmsAbvGrd|Functional|Fireplaces|FireplaceQu|GarageType|GarageYrBlt|GarageFinish|GarageCars|GarageArea|GarageQual|GarageCond|PavedDrive|WoodDeckSF|OpenPorchSF|EnclosedPorch|3SsnPorch|ScreenPorch|PoolArea|PoolQC|Fence|MiscFeature|MiscVal|MoSold|YrSold|SaleType|SaleCondition|SalePrice|MorePrecisionSalePrice|\n",
      "+---+----------+--------+-----------+-------+------+-----+--------+-----------+---------+---------+---------+------------+----------+----------+--------+----------+-----------+-----------+---------+------------+---------+--------+-----------+-----------+----------+----------+---------+---------+----------+--------+--------+------------+------------+----------+------------+----------+---------+-----------+-------+---------+----------+----------+--------+--------+------------+---------+------------+------------+--------+--------+------------+------------+-----------+------------+----------+----------+-----------+----------+-----------+------------+----------+----------+----------+----------+----------+----------+-----------+-------------+---------+-----------+--------+------+-----+-----------+-------+------+------+--------+-------------+---------+----------------------+\n",
      "|  1|        60|      RL|         65|   8450|  Pave|   NA|     Reg|        Lvl|   AllPub|   Inside|      Gtl|     CollgCr|      Norm|      Norm|    1Fam|    2Story|          7|          5|     2003|        2003|    Gable| CompShg|    VinylSd|    VinylSd|   BrkFace|       196|       Gd|       TA|     PConc|      Gd|      TA|          No|         GLQ|       706|         Unf|         0|      150|        856|   GasA|       Ex|         Y|     SBrkr|     856|     854|           0|     1710|           1|           0|       2|       1|           3|           1|         Gd|           8|       Typ|         0|         NA|    Attchd|       2003|         RFn|         2|       548|        TA|        TA|         Y|         0|         61|            0|        0|          0|       0|    NA|   NA|         NA|      0|     2|  2008|      WD|       Normal|   208500|              208500.0|\n",
      "|  2|        20|      RL|         80|   9600|  Pave|   NA|     Reg|        Lvl|   AllPub|      FR2|      Gtl|     Veenker|     Feedr|      Norm|    1Fam|    1Story|          6|          8|     1976|        1976|    Gable| CompShg|    MetalSd|    MetalSd|      None|         0|       TA|       TA|    CBlock|      Gd|      TA|          Gd|         ALQ|       978|         Unf|         0|      284|       1262|   GasA|       Ex|         Y|     SBrkr|    1262|       0|           0|     1262|           0|           1|       2|       0|           3|           1|         TA|           6|       Typ|         1|         TA|    Attchd|       1976|         RFn|         2|       460|        TA|        TA|         Y|       298|          0|            0|        0|          0|       0|    NA|   NA|         NA|      0|     5|  2007|      WD|       Normal|   181500|              181500.0|\n",
      "|  3|        60|      RL|         68|  11250|  Pave|   NA|     IR1|        Lvl|   AllPub|   Inside|      Gtl|     CollgCr|      Norm|      Norm|    1Fam|    2Story|          7|          5|     2001|        2002|    Gable| CompShg|    VinylSd|    VinylSd|   BrkFace|       162|       Gd|       TA|     PConc|      Gd|      TA|          Mn|         GLQ|       486|         Unf|         0|      434|        920|   GasA|       Ex|         Y|     SBrkr|     920|     866|           0|     1786|           1|           0|       2|       1|           3|           1|         Gd|           6|       Typ|         1|         TA|    Attchd|       2001|         RFn|         2|       608|        TA|        TA|         Y|         0|         42|            0|        0|          0|       0|    NA|   NA|         NA|      0|     9|  2008|      WD|       Normal|   223500|              223500.0|\n",
      "+---+----------+--------+-----------+-------+------+-----+--------+-----------+---------+---------+---------+------------+----------+----------+--------+----------+-----------+-----------+---------+------------+---------+--------+-----------+-----------+----------+----------+---------+---------+----------+--------+--------+------------+------------+----------+------------+----------+---------+-----------+-------+---------+----------+----------+--------+--------+------------+---------+------------+------------+--------+--------+------------+------------+-----------+------------+----------+----------+-----------+----------+-----------+------------+----------+----------+----------+----------+----------+----------+-----------+-------------+---------+-----------+--------+------+-----+-----------+-------+------+------+--------+-------------+---------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "house_prices_df.withColumn(\"MorePrecisionSalePrice\", morePrecisionUdf('SalePrice)).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying some transformation on all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  x|  y|  z|\n",
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "|  D|  E|  F|\n",
      "+---+---+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [x: string, y: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[x: string, y: string ... 1 more field]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, upper}\n",
    "\n",
    "val df = sc.parallelize(\n",
    "  Seq((\"a\", \"B\", \"c\"), (\"D\", \"e\", \"F\"))).toDF(\"x\", \"y\", \"z\")\n",
    "df.select(df.columns.map(c => upper(col(c)).alias(c)): _*).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A look at datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Datasets API provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. You can define a Dataset JVM objects and then manipulate them using functional transformations (map, flatMap, filter, and so on) similar to an RDD. The benefits is that, unlike RDDs, these transformations are now applied on a structured and strongly typed distributed collection that allows Spark to leverage Spark SQL’s execution engine for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "| value|numOccurances|\n",
      "+------+-------------+\n",
      "| spark|            3|\n",
      "|father|            2|\n",
      "|  your|            2|\n",
      "|    am|            2|\n",
      "|     i|            2|\n",
      "|  with|            1|\n",
      "|   may|            1|\n",
      "|   you|            1|\n",
      "|    be|            1|\n",
      "|   the|            1|\n",
      "+------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wordsDataset = [value: string]\n",
       "result = [value: string, numOccurances: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[value: string, numOccurances: bigint]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val wordsDataset = sc.parallelize(Seq(\"Spark I am your father\", \"May the spark be with you\", \"Spark I am your father\")).toDS()\n",
    "val result = wordsDataset\n",
    "              .flatMap(_.split(\" \"))               // Split on whitespace\n",
    "              .filter(_ != \"\")                     // Filter empty words\n",
    "              .map(_.toLowerCase())\n",
    "              .toDF()                              // Convert to DataFrame to perform aggregation / sorting\n",
    "              .groupBy($\"value\")                   // Count number of occurrences of each word\n",
    "              .agg(count(\"*\") as \"numOccurances\")\n",
    "              .orderBy($\"numOccurances\" desc)      // Show most common words first\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Capabilities: Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userSchema = StructType(StructField(Suburb,StringType,true), StructField(Address,StringType,true))\n",
       "csvDF = [Suburb: string, Address: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Suburb: string, Address: string]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.sql.types.{StringType, StructType, StructField, IntegerType}\n",
    "\n",
    "val userSchema = new StructType().add(\"Suburb\", \"string\").add(\"Address\", \"string\")\n",
    "val csvDF = spark\n",
    "  .readStream\n",
    "  .option(\"sep\", \",\")\n",
    "  .schema(userSchema)      // Specify schema of the csv files\n",
    "  .option(\"maxFilesPerTrigger\", 1)\n",
    "  .csv(\"/home/jovyan/data/melbourne_housing_snapshot/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "streamingCountsDF = [Suburb: string, window: struct<start: timestamp, end: timestamp> ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Same query as staticInputDF\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val streamingCountsDF = \n",
    "  csvDF\n",
    "    .groupBy($\"Suburb\", window($\"Address\", \"1 minute\"))\n",
    "    .count()\n",
    "\n",
    "streamingCountsDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1060be6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1060be6"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")  // keep the size of shuffles small\n",
    "\n",
    "val query =\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")        // memory = store in-memory table (for testing only in Spark 2.0)\n",
    "    .queryName(\"counts\")     // counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")  // complete = all the counts should be in the table\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "|Suburb|window|count|\n",
      "+------+------+-----+\n",
      "+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from counts\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A sample machine learning dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bike_sharing_df = [instant: int, dteday: timestamp ... 15 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[instant: int, dteday: timestamp ... 15 more fields]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val bike_sharing_df = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", true)\n",
    "    .load(\"/home/jovyan/data/bike-sharing/hour.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureCols = Array(season, yr, mnth, hr, holiday, weekday, workingday, weathersit, temp, atemp, hum, windspeed, cnt)\n",
       "assembler = vecAssembler_e03f82143a97\n",
       "dataDF = [instant: int, dteday: timestamp ... 16 more fields]\n",
       "dataDF1 = [instant: int, dteday: timestamp ... 16 more fields]\n",
       "train = [instant: int, dteday: timestamp ... 16 more fields]\n",
       "test = [instant: int, dteday: timestamp ... 16 more fields]\n",
       "lr = linReg_4e3773869c3c\n",
       "lrModel = linReg_4e3773869c3c\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "linReg_4e3773869c3c"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureCols = Array(\"season\", \"yr\", \"mnth\", \"hr\", \n",
    "                        \"holiday\", \"weekday\", \"workingday\",\n",
    "                        \"weathersit\", \"temp\", \"atemp\",\n",
    "                        \"hum\", \"windspeed\", \"cnt\")\n",
    "val assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol(\"features\")\n",
    "val dataDF = assembler.transform(bike_sharing_df)\n",
    "val dataDF1 = dataDF.withColumnRenamed(\"cnt\", \"label\")\n",
    "\n",
    "val Array(train, test) = dataDF1.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setMaxIter(1000)\n",
    "    .setRegParam(0.3)\n",
    "    .setElasticNetParam(0.8)\n",
    "\n",
    "//Fit the model\n",
    "val lrModel = lr.fit(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
