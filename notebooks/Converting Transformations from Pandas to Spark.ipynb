{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to setup the spark context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the latest spark https://www.apache.org/dyn/closer.lua/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go Inside and run the spark-shell command. This will download all the relevant jars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SparkContext is a client of Spark’s execution environment and it acts as the master of the Spark application. SparkContext sets up internal services and establishes a connection to a Spark execution environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to avoid hard-coding certain configurations in a SparkConf. For instance, if you’d like to run the same application with different masters or different amounts of memory. Spark allows you to simply create an empty conf:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    val sc = new SparkContext(new SparkConf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you can supply configuration values at runtime:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ./bin/spark-submit --name \"My app\" --master local[4] --conf spark.eventLog.enabled=false --conf \"spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\" myApp.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick word on spark tools: sbt, and spark-submit, spark-shell, pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sbt is the built tool for building scala applications.\n",
    "\n",
    "You will need to submit applications to your spark cluster using the spark-submit.\n",
    "\n",
    "Spark-shell will help you in understanding the code execution flow. It is similar to ipython for spark.\n",
    "\n",
    "FInally the bigger question of whether to use scala. According to me the question is do you already have a lot of legacy code in python, and how comfortable is your team to go into the typed environment of scala. Do you believe that strict typing is your friend, because that will be an extra cognitive load. If you ask me, making the upfront investment in typing will help you in your data debugging process. I highly recommend this is the data that you are getting is ambiguous and is likely to change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between a transformation and action\n",
    "\n",
    "In pandas everything is a transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations are executed on demand.(Lazy computation)\n",
    "Ex: filter(), union()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Action will return a non-RDD type (your stored value types usually)\n",
    "Actions triggers execution using lineage graph to load the data into original RDD\n",
    "Ex: count(), first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference: creating a pandas DF and a Spark DF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames generally refer to a data structure, which is tabular in nature. It represents rows, each of which consists of a number of observations. Rows can have a variety of data formats (heterogeneous), whereas a column can have data of the same data type (homogeneous). DataFrames usually contain some metadata in addition to data; for example, column and row names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas:\n",
    "\n",
    "![title](img/pandas_read_csv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val house_prices_df = spark.read\n",
    "    .format(\"csv\")                                    // this is a csv file.\n",
    "    .option(\"header\", \"true\")                         // the file contains headers\n",
    "    .option(\"inferSchema\", true)                      // read the schema\n",
    "    .load(\"/home/jovyan/data/house-prices/train.csv\") // now load the file.\n",
    "\n",
    "val melb_data = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", true)\n",
    "    .load(\"/home/jovyan/data/melbourne_housing_snapshot/melb_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of Spark Dataframes\n",
    "\n",
    "    * DataFrames are distributed in nature, which makes it a fault tolerant and highly available data structure.\n",
    "    * Lazy evaluation is an evaluation strategy which holds the evaluation of an expression until its value is needed. It avoids repeated evaluation. Lazy evaluation in Spark means that the execution will not start until an action is triggered. In Spark, the picture of lazy evaluation comes when Spark transformations occur.\n",
    "    * DataFrames are immutable in nature. By immutable, I mean that it is an object whose state cannot be modified after it is created. But we can transform its values by applying a certain transformation, like in RDDs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing a particular column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![Describing in pandas](./img/pandas_describe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df.describe(\"MSSubClass\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataframe shape and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/pandas_shape_columns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the number of samples is an action and hence be mindful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the number of features is a no big deal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df\n",
    "    .columns\n",
    "    .size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the column names of the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/change_column_names.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = Seq((1L, \"a\", \"foo\", 3.0)).toDF\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val newNames = Seq(\"id\", \"x1\", \"x2\", \"x3\")\n",
    "val dfRenamed = df.toDF(newNames: _*)\n",
    "\n",
    "dfRenamed.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val lookup = Map(\"Id\" -> \"id\", \"SalePrice\" -> \"SalePriceDollars\")\n",
    "\n",
    "val changed_cols_df = house_prices_df.select(\n",
    "    house_prices_df.columns\n",
    "    .map(\n",
    "        c => col(c).as(lookup.getOrElse(c, c))\n",
    "    ): _*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_cols_df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/unique_values.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df\n",
    "    .select(\"MSSubClass\")\n",
    "    .distinct()\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large data when you are only interested in the count of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.countDistinct\n",
    "\n",
    "house_prices_df.select(countDistinct('MSSubClass)).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do count approximate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Counts\n",
    "\n",
    "ref: https://stackoverflow.com/a/37949565/5417164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/value_counts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "house_prices_df\n",
    "    .groupBy(\"MSSubClass\")  // groupby your class\n",
    "    .count()                // count the values, this should create a dedicated count column\n",
    "    .orderBy($\"count\" desc)       // orderby the count column\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by and group by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/orderbuy_grpby.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "house_prices_df\n",
    "    .groupBy($\"MSSubClass\")                   // Count number of occurrences of each word\n",
    "    .agg(count(\"*\") as \"numOccurances\")       // SQL: SELECT COUNT(DISTINCT MSSubClass) AS numOccurances FROM house_prices_df\n",
    "    .orderBy($\"numOccurances\" desc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/filtering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val highSubClass = house_prices_df\n",
    "    .filter($\"MSSubClass\" > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highSubClass.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membership in dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/membership.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val presentList = List(\"20\",\"60\") \n",
    "val nopresentList = List(\"20000\") \n",
    "val validMembership = house_prices_df\n",
    "    .filter($\"MSSubClass\"\n",
    "            .isin(presentList:_*))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val invalidMembership = house_prices_df\n",
    "    .filter($\"MSSubClass\"\n",
    "            .isin(nopresentList:_*))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the count below is the sum of 536 + 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validMembership.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalidMembership.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value imputation\n",
    "\n",
    "refs: https://stackoverflow.com/a/40059453/5417164 \n",
    "\n",
    "https://medium.com/@mrpowers/dealing-with-null-in-spark-cfdbb12f231e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melb_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melb_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/fillna.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to pandas you can replace the na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val imputed_melb_data = melb_data\n",
    "    .na\n",
    "    .fill(1964.0, Seq(\"YearBuilt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_melb_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/imputer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Imputer\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val features_in_focus = Array(\"Rooms\", \"Bathroom\", \"Landsize\", \"BuildingArea\",\n",
    "                              \"YearBuilt\", \"Lattitude\", \"Longtitude\")\n",
    "val features_in_focus_imputed = features_in_focus.map(c => s\"${c}_imputed\")\n",
    "\n",
    "val imputer = new Imputer()\n",
    "  .setInputCols(features_in_focus)\n",
    "  .setOutputCols(features_in_focus_imputed)\n",
    "  .setStrategy(\"mean\")\n",
    "\n",
    "val imputed_melb_data = imputer.fit(melb_data).transform(melb_data)\n",
    "// val imputed_melb_data = imputer.fit(melb_data2).transform(melb_data2)\n",
    "\n",
    "imputed_melb_data.select(features_in_focus_imputed.map(name => col(name)):_*).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above the features must be for double type or floattype. But the Rooms feature is of type Integer and hence we will need to convert that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.DoubleType\n",
    "\n",
    "val melb_data2 = melb_data\n",
    "    .withColumn(\"_Rooms\", melb_data(\"Rooms\").cast(DoubleType))\n",
    "    .drop(\"Rooms\")\n",
    "    .withColumnRenamed(\"_Rooms\", \"Rooms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Imputer\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val features_in_focus = Array(\"Rooms\", \"Bathroom\", \"Landsize\", \"BuildingArea\",\n",
    "                              \"YearBuilt\", \"Lattitude\", \"Longtitude\")\n",
    "val features_in_focus_imputed = features_in_focus.map(c => s\"${c}_imputed\")\n",
    "\n",
    "val imputer = new Imputer()\n",
    "  .setInputCols(features_in_focus)\n",
    "  .setOutputCols(features_in_focus_imputed)\n",
    "  .setStrategy(\"mean\")\n",
    "\n",
    "val imputed_melb_data = imputer.fit(melb_data2).transform(melb_data2)\n",
    "\n",
    "imputed_melb_data.select(features_in_focus_imputed.map(name => col(name)):_*).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization and Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/binning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Bucketizer\n",
    "\n",
    "val splits = Array(Double.NegativeInfinity, -0.5, 0.0, 0.5, Double.PositiveInfinity)\n",
    "\n",
    "val data = Array(-999.9, -0.5, -0.3, 0.0, 0.2, 999.9)\n",
    "val dataFrame = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "\n",
    "println(\"showing the dataframe\")\n",
    "dataFrame.show()\n",
    "\n",
    "val bucketizer = new Bucketizer()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"bucketedFeatures\")\n",
    "  .setSplits(splits)\n",
    "\n",
    "// Transform original data into its bucket index.\n",
    "val bucketedData = bucketizer.transform(dataFrame)\n",
    "\n",
    "println(s\"Bucketizer output with ${bucketizer.getSplits.length-1} buckets\")\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = Array(20.0, 22.0, 25.0, 27.0, 21.0, 23.0, 37.0, 31.0, 61.0, 45.0, 41.0, 32.0)\n",
    "val bins = Array(18.0, 25.0, 35.0, 60.0, 100.0)\n",
    "\n",
    "// val dataFrame = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "val dataFrame = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "\n",
    "println(\"showing the dataframe\")\n",
    "dataFrame.show()\n",
    "\n",
    "val bucketizer = new Bucketizer()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"bucketedFeatures\")\n",
    "  .setSplits(bins)\n",
    "\n",
    "// Transform original data into its bucket index.\n",
    "val bucketedData = bucketizer.transform(dataFrame)\n",
    "\n",
    "println(s\"Bucketizer output with ${bucketizer.getSplits.length-1} buckets\")\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a particular data\n",
    "\n",
    "ref: https://stackoverflow.com/a/35720457/5417164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/iloc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = house_prices_df.\n",
    "    filter(line => line(0) == 1)\n",
    "    .select(\"MSSubClass\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting\n",
    "\n",
    "ref: https://stackoverflow.com/a/32052881/5417164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/sorting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val sortedbyMsSubclass = house_prices_df\n",
    "    .sort(\n",
    "        desc(\"MSSubClass\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val columnNames = Seq(\"MSSubClass\", \"SalePrice\")\n",
    "sortedbyMsSubclass.select(columnNames.map(c => col(c)): _*).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping and Pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/pivot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// create RDD for products\n",
    "val data = sc.parallelize(Seq(\n",
    "    (\"memories\",\"book\",\"q1\",10),\n",
    "    (\"dreams\",\"book\",\"q2\",20),\n",
    "    (\"reflections\",\"book\",\"q3\",30),\n",
    "    (\"how to build a house\",\"book\",\"q4\",40),\n",
    "    (\"wonderful life\",\"music\",\"q1\",10),\n",
    "    (\"million miles\",\"music\",\"q2\",20),\n",
    "    (\"run away\",\"music\",\"q3\",30),\n",
    "    (\"mind and body\",\"music\",\"q4\",40)\n",
    "))\n",
    "\n",
    "// convert the RDD to DataFrame\n",
    "val df_products = spark.createDataFrame(data).toDF(\"product\",\"category\",\"quarter\",\"profit\")\n",
    "df_products.show()\n",
    "\n",
    "// index column : category\n",
    "// value column : profit\n",
    "// pivot column : quarter\n",
    "// agg function : sum\n",
    "\n",
    "// apply pivot on DataFrame DataFrame\n",
    "df_products\n",
    "    .groupBy(\"category\")\n",
    "    .pivot(\"quarter\")\n",
    "    .sum(\"profit\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merges and Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/merges.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val llist = Seq((\"bob\", \"2015-01-13\", 4), (\"alice\", \"2015-04-23\",10))\n",
    "val left = llist.toDF(\"name\",\"date\",\"duration\")\n",
    "val right = Seq((\"alice\", 100),(\"bob\", 23)).toDF(\"name\",\"upload\")\n",
    "\n",
    "val df = left\n",
    "    .join(right, Seq(\"name\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating and appending to the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "![](./img/append.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val llist = Seq((\"bob\", \"2015-01-13\", 4), (\"alice\", \"2015-04-23\",10))\n",
    "val arr = llist.toDF(\"name\",\"date\",\"duration\")\n",
    "val appended = arr.union(\n",
    "    arr.toDF())\n",
    "appended.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function application, transformations and mapping\n",
    "\n",
    "By using user defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morePrecision(price: Integer): Float = price.toFloat\n",
    "\n",
    "// we use the method name followed by a \"_\" to indicate we want a reference\n",
    "// to the method, not call it\n",
    "val morePrecisionUdf = udf(morePrecision _)\n",
    "\n",
    "val converted_df = house_prices_df.select(\n",
    "    morePrecisionUdf(house_prices_df(\"SalePrice\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the udf on the same df. ie. creating a new feature by transforming another column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices_df\n",
    "    .withColumn(\"MorePrecisionSalePrice\", morePrecisionUdf('SalePrice))\n",
    "    .show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying some transformation on all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{col, upper}\n",
    "\n",
    "val df = sc.parallelize(\n",
    "  Seq((\"a\", \"B\", \"c\"), (\"D\", \"e\", \"F\"))).toDF(\"x\", \"y\", \"z\")\n",
    "df.select(df.columns.map(c => upper(col(c)).alias(c)): _*).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A look at datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Datasets API provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. You can define a Dataset JVM objects and then manipulate them using functional transformations (map, flatMap, filter, and so on) similar to an RDD. The benefits is that, unlike RDDs, these transformations are now applied on a structured and strongly typed distributed collection that allows Spark to leverage Spark SQL’s execution engine for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val wordsDataset = sc.parallelize(Seq(\"Spark I am your father\", \"May the spark be with you\", \"Spark I am your father\")).toDS()\n",
    "val result = wordsDataset\n",
    "              .flatMap(_.split(\" \"))               // Split on whitespace\n",
    "              .filter(_ != \"\")                     // Filter empty words\n",
    "              .map(_.toLowerCase())\n",
    "              .toDF()                              // Convert to DataFrame to perform aggregation / sorting\n",
    "              .groupBy($\"value\")                   // Count number of occurrences of each word\n",
    "              .agg(count(\"*\") as \"numOccurances\")\n",
    "              .orderBy($\"numOccurances\" desc)      // Show most common words first\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Capabilities: Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StringType, StructType, StructField, IntegerType}\n",
    "\n",
    "val userSchema = new StructType().add(\"Suburb\", \"string\").add(\"Address\", \"string\")\n",
    "val csvDF = spark\n",
    "  .readStream\n",
    "  .option(\"sep\", \",\")\n",
    "  .schema(userSchema)      // Specify schema of the csv files\n",
    "  .option(\"maxFilesPerTrigger\", 1)\n",
    "  .csv(\"/home/jovyan/data/melbourne_housing_snapshot/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Same query as staticInputDF\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val streamingCountsDF = \n",
    "  csvDF\n",
    "    .groupBy($\"Suburb\", window($\"Address\", \"1 minute\"))\n",
    "    .count()\n",
    "\n",
    "streamingCountsDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")  // keep the size of shuffles small\n",
    "\n",
    "val query =\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")        // memory = store in-memory table (for testing only in Spark 2.0)\n",
    "    .queryName(\"counts\")     // counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")  // complete = all the counts should be in the table\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from counts\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A sample machine learning dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val bike_sharing_df = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", true)\n",
    "    .load(\"/home/jovyan/data/bike-sharing/hour.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val featureCols = Array(\"season\", \"yr\", \"mnth\", \"hr\", \n",
    "                        \"holiday\", \"weekday\", \"workingday\",\n",
    "                        \"weathersit\", \"temp\", \"atemp\",\n",
    "                        \"hum\", \"windspeed\", \"cnt\")\n",
    "val assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol(\"features\")\n",
    "val dataDF = assembler.transform(bike_sharing_df)\n",
    "val dataDF1 = dataDF.withColumnRenamed(\"cnt\", \"label\")\n",
    "\n",
    "val Array(train, test) = dataDF1.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setMaxIter(1000)\n",
    "    .setRegParam(0.3)\n",
    "    .setElasticNetParam(0.8)\n",
    "\n",
    "//Fit the model\n",
    "val lrModel = lr.fit(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
